services:
  gbif.api.url: https://___DONT_USE______xapi.gbif.org
  ala_taxonomy.api.url: http://localhost:9179
  ala_spatial.api.url: http://localhost:8080
  ala_collectory.api.url: https://collections.ala.org.au
  ala_lists.api.url: http://lists.ala.org.au
  geocode.api.url: http://127.0.0.1:4444/geocode/%

general:
  targetPath: /data/pipelines-data
  attempt: 1
  hdfsSiteConfig: ""
  coreSiteConfig: ""

dwca-avro:
  runner: SparkRunner
  metaFileName: dwca-metrics.yml
  inputPath: /data/biocache-load/{datasetId}

interpret:
  interpretationTypes: ALL
  inputPath: /data/pipelines-data/{datasetId}/1/verbatim.avro
  metaFileName: interpretation-metrics.yml
  useExtendedRecordId: true
  runner: SparkRunner
  # TODO improve/move this
  properties: ../scripts/pipelines.yaml
  ## For spark-cluster:
  #  name: interpret {datasetId}
  #  appName: Interpretation for {datasetId}
  ## For spark-embedded:
  #  appName: Interpretation for {datasetId}

export-latlng:
  inputPath: /data/pipelines-data
  runner: SparkRunner
  appName: Lat Long export for {datasetId}

sample:
  inputPath: /data/pipelines-data
  appName: Sample for {datasetId}
  metaFileName: indexing-metrics.yml
  runner: SparkRunner
  # TODO improve/move this
  properties: ../scripts/pipelines.yaml

sample-avro:
  inputPath: /data/pipelines-data
  runner: SparkRunner
  metaFileName: indexing-metrics.yml
  # TODO improve/move this
  properties: ../scripts/pipelines.yaml
  ## For spark-cluster:
  # appName: Add Sampling for {datasetId}
  # useExtendedRecordId: true
  ## For spark-embedded:
  # appName: SamplingToAvro indexing for {datasetId}

migrate-uuids:
  default:
    runner: SparkRunner
    metaFileName: uuid-metrics.yml
    targetPath: /data/pipelines-data
    inputPath: /data/pipelines-data/occ_uuid.csv
  #java -Xmx24g -Xmx24g -XX:+UseG1GC
  test:
    runner: DirectRunner

# java -Xmx8g -Xmx8g -XX:+UseG1GC
uuid:
  embedded:
    appName: UUID minting for {datasetId}
    runner: SparkRunner
    inputPath: /data/pipelines-data
    metaFileName: uuid-metrics.yml
#--properties=pipelines.properties

#java -Xmx8g -XX:+UseG1GC -
index:
  default:
    inputPath: /data/pipelines-data
    metaFileName: indexing-metrics.yml
  java:
    #--properties=pipelines.properties \
    includeSampling: true
    zkHost: localhost:9983
    solrCollection: biocache
  spark-cluster:
    # TODO
  # java -Xmx8g -Xmx8g -XX:+UseG1GC
  spark-embedded:
    appName: SOLR indexing for {datasetId}
    runner: SparkRunner
#    properties: pipelines.properties
    zkHost: localhost:9983
    solrCollection: biocache
    includeSampling: true
