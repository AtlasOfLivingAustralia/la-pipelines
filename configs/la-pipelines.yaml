services:
  gbif.api.url: https://___DONT_USE______xapi.gbif.org
  ala_taxonomy.api.url: http://localhost:9179
  ala_spatial.api.url: http://localhost:8080
  ala_collectory.api.url: https://collections.ala.org.au
  ala_lists.api.url: http://lists.ala.org.au
  geocode.api.url: http://127.0.0.1:4444/geocode/%

general:
  targetPath: /data/pipelines-data
  attempt: 1
  hdfsSiteConfig: ""
  coreSiteConfig: ""

dwca-avro:
  runner: SparkRunner
  metaFileName: dwca-metrics.yml
  inputPath: /data/biocache-load/{datasetId}

interpret:
  interpretationTypes: ALL
  inputPath: /data/pipelines-data/{datasetId}/1/verbatim.avro
  metaFileName: interpretation-metrics.yml
  useExtendedRecordId: true
  runner: SparkRunner
  # TODO improve/move this
  properties: ../scripts/pipelines.yaml
  ## For spark-cluster:
  #  name: interpret {datasetId}
  #  appName: Interpretation for {datasetId}
  ## For spark-embedded:
  #  appName: Interpretation for {datasetId}

uuid:
  appName: UUID minting for {datasetId}
  runner: SparkRunner
  inputPath: /data/pipelines-data
  metaFileName: uuid-metrics.yml
  properties: ../scripts/pipelines.yaml
  ## For spark-cluster:
  # interpretationTypes: ALL
  # useExtendedRecordId: true
  # skipRegisrtyCalls: true

export-latlng:
  inputPath: /data/pipelines-data
  runner: SparkRunner
  appName: Lat Long export for {datasetId}

sample:
  inputPath: /data/pipelines-data
  appName: Sample for {datasetId}
  metaFileName: indexing-metrics.yml
  runner: SparkRunner
  # TODO improve/move this
  properties: ../scripts/pipelines.yaml

sample-avro:
  inputPath: /data/pipelines-data
  runner: SparkRunner
  metaFileName: indexing-metrics.yml
  # TODO improve/move this
  properties: ../scripts/pipelines.yaml
  ## For spark-cluster:
  # appName: Add Sampling for {datasetId}
  # useExtendedRecordId: true
  ## For spark-embedded:
  # appName: SamplingToAvro indexing for {datasetId}

migrate-uuids:
  inputPath: /data/pipelines-data/occ_uuid.csv
  targetPath: /data/pipelines-data
  hdfsSiteConfig: ""
  # FIXME: MigrateUUIDPipeline should use this also?

index:
  inputPath: /data/pipelines-data
  metaFileName: indexing-metrics.yml
  properties: ../scripts/pipelines.yaml
  solrCollection: biocache
  includeSampling: true
  zkHost: localhost:9983
  ## For spark-cluster:
  # appName: SOLR indexing for {datasetId}
  # runner: SparkRunner
  ## For spark-embedded:
  # appName: SOLR indexing for {datasetId}
  # runner: SparkRunner
